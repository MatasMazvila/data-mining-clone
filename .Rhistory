kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
silhouette_result <- silhouette(kmeans_result$cluster, dist(kmeans_data[, 1:2]))
mean(silhouette_result[, 3])  # Average silhouette width
})
# Plot Silhouette Scores
plot(k_values, silhouette_scores_means, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for K-Means")
# Gap Statistic for K-Means
gap_stat_means <- clusGap(kmeans_data[, 1:2], FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Gap Statistic for K-Means
gap_stat_means <- clusGap(kmeans_data[, 1:2], FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat_means)
# Inspect Gap Statistic
print(gap_stat_means)
num_clusters <- 6
# Add cluster labels to the dataset
kmeans_data$cluster <- as.factor(kmeans_result$cluster)
# Visualize the clusters
ggplot(kmeans_data, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "K-Means Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Silhouette Analysis for K-Means
silhouette_scores_means <- sapply(k_values, function(k) {
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
silhouette_result <- silhouette(kmeans_result$cluster, dist(kmeans_data[, 1:2]))
mean(silhouette_result[, 3])  # Average silhouette width
})
# Plot Silhouette Scores
plot(k_values, silhouette_scores_means, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for K-Means")
# Gap Statistic for K-Means
gap_stat_means <- clusGap(kmeans_data[, 1:2], FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat_means)
# Inspect Gap Statistic
print(gap_stat_means)
num_clusters <- 6
# Add cluster labels to the dataset
kmeans_data$cluster <- as.factor(kmeans_result$cluster)
k_values <- 2:15
wss_means <- sapply(k_values, function(k) {
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
kmeans_result$tot.withinss
})
# Add cluster labels to the dataset
kmeans_data$cluster <- as.factor(kmeans_result$cluster)
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
# Elbow Method for K-Means
set.seed(123)
k_values <- 2:15
wss_means <- sapply(k_values, function(k) {
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 10)
kmeans_result$tot.withinss
})
# Plot the Elbow Curve
plot(k_values, wss_means, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Total Within-Cluster Sum of Squares (WSS)",
main = "Elbow Method for K-Means")
# Elbow Method for K-Means
set.seed(123)
k_values <- 2:15
wss_means <- sapply(k_values, function(k) {
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 1000)
kmeans_result$tot.withinss
})
# Plot the Elbow Curve
plot(k_values, wss_means, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Total Within-Cluster Sum of Squares (WSS)",
main = "Elbow Method for K-Means")
# Silhouette Analysis for K-Means
silhouette_scores_means <- sapply(k_values, function(k) {
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
silhouette_result <- silhouette(kmeans_result$cluster, dist(kmeans_data[, 1:2]))
mean(silhouette_result[, 3])  # Average silhouette width
})
# Plot Silhouette Scores
plot(k_values, silhouette_scores_means, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for K-Means")
num_clusters <- 6
# Add cluster labels to the dataset
kmeans_data$cluster <- as.factor(kmeans_result$cluster)
# Visualize the clusters
ggplot(kmeans_data, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "K-Means Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Run K-means
num_clusters <- 6
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = num_clusters, nstart = 25)
# Add cluster labels to the dataset
kmeans_data$cluster <- as.factor(kmeans_result$cluster)
# Visualize the clusters
ggplot(kmeans_data, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "K-Means Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Run K-means
num_clusters <- 7
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = num_clusters, nstart = 25)
# Add cluster labels to the dataset
kmeans_data$cluster <- as.factor(kmeans_result$cluster)
# Visualize the clusters
ggplot(kmeans_data, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "K-Means Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Visualize the clusters
ggplot(kmeans_data, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "K-Means Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Run K-means
num_clusters <- 6
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = num_clusters, nstart = 25)
# Method 1: k-Distance Plot to Find Optimal `eps`
k <- 6  # Typically, k = minPts - 1
# Method 1: k-Distance Plot to Find Optimal `eps`
k <- 6  # Typically, k = minPts - 1
knn_distances <- kNNdist(similarity_matrix, k = k)
# Plot k-distance to find the "elbow"
plot(
sort(knn_distances), type = "l", col = "blue", lwd = 2,
xlab = "Points sorted by distance",
ylab = "k-NN Distance",
main = "k-Distance Plot for DBSCAN"
)
# Based on the elbow in the plot, choose a value for `eps`
eps_value <- 1.36  # Adjust based on the elbow in the k-distance plot
min_points <- 7    # Typical values are between 3 and 10
# Perform DBSCAN
dbscan_result <- dbscan(distance_matrix, eps = eps_value, minPts = min_points)
# Add cluster labels to the dataset
commenter_jaccard$dbscan_cluster <- dbscan_result$cluster
# Prepare data for visualization
visualization_data_dbscan <- data.frame(
X1 = mds_coords[, 1],
X2 = mds_coords[, 2],
cluster = as.factor(dbscan_result$cluster)
)
# Plot the clustering result
ggplot(visualization_data_dbscan, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
labs(
title = "DBSCAN Clustering Visualization",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Testing min points and eps values (we probably need a bigged data frame)
for (minPts_test in 1:4) {
for (eps_test in seq(0.5, 17, by = 0.5)) {
test_result <- dbscan(similarity_matrix, eps = eps_test, minPts = minPts_test)
cat("\nDBSCAN with eps =", eps_test, "and minPts =", minPts_test, "\n")
print(table(test_result$cluster))
}
}
# Testing min points and eps values (we probably need a bigged data frame)
for (minPts_test in 1:4) {
for (eps_test in seq(0.5, 2, by = 0.05)) {
test_result <- dbscan(similarity_matrix, eps = eps_test, minPts = minPts_test)
cat("\nDBSCAN with eps =", eps_test, "and minPts =", minPts_test, "\n")
print(table(test_result$cluster))
}
}
############## K-Medoids Clustering ##############
# Define similarity matrix
similarity_matrix <- as.matrix(commenter_jaccard[, -1])
# Ensure the similarity matrix is symmetric
similarity_matrix <- (similarity_matrix + t(similarity_matrix)) / 2
# Remove dimnames to avoid warnings
rownames(similarity_matrix) <- NULL
colnames(similarity_matrix) <- NULL
# Convert similarity matrix to a distance matrix
distance_matrix <- as.dist(1 - similarity_matrix) %>%
as.matrix()
# Elbow Method to Determine Optimal k
k_values <- 2:15
channel_info <- read_csv("data/channel_info.csv")
commenter_jaccard <- read_csv("data/comment_jaccard_matrix.csv")
############## K-Medoids Clustering ##############
# Define similarity matrix
similarity_matrix <- as.matrix(commenter_jaccard[, -1])
# Ensure the similarity matrix is symmetric
similarity_matrix <- (similarity_matrix + t(similarity_matrix)) / 2
# Remove dimnames to avoid warnings
rownames(similarity_matrix) <- NULL
colnames(similarity_matrix) <- NULL
# Convert similarity matrix to a distance matrix
distance_matrix <- as.dist(1 - similarity_matrix) %>%
as.matrix()
channel_info <- read_csv("data/channel_info.csv")
commenter_jaccard <- read_csv("data/comment_jaccard_matrix.csv")
# Define similarity matrix
similarity_matrix <- as.matrix(commenter_jaccard[, -1])
# Ensure the similarity matrix is symmetric
similarity_matrix <- (similarity_matrix + t(similarity_matrix)) / 2
# Remove dimnames to avoid warnings
rownames(similarity_matrix) <- NULL
colnames(similarity_matrix) <- NULL
# Convert similarity matrix to a distance matrix
distance_matrix <- as.dist(1 - similarity_matrix) %>%
as.matrix()
# Multidimensional Scaling (MDS) for Visualization
mds_coords <- cmdscale(distance_matrix, k = 2)
# Compute Hopkins Statistic
set.seed(123)
hopkins_stat <- hopkins(mds_coords)
library(hopkins)
install.packages("hopkins")
# Perform hierarchical clustering using Ward's method
hc_result <- hclust(distance_matrix, method = "ward.D2")
# Define similarity matrix
similarity_matrix <- as.matrix(commenter_jaccard[, -1])
# Ensure the similarity matrix is symmetric
similarity_matrix <- (similarity_matrix + t(similarity_matrix)) / 2
# Remove dimnames to avoid warnings
rownames(similarity_matrix) <- NULL
colnames(similarity_matrix) <- NULL
distance_matrix <- as.dist(1 - similarity_matrix)
# Multidimensional Scaling (MDS) for Visualization
mds_coords <- cmdscale(distance_matrix, k = 2)
# Compute Hopkins Statistic
set.seed(123)
hopkins_stat <- hopkins(mds_coords)
# Print the Hopkins Statistic
print(paste("Hopkins Statistic:", round(hopkins_stat, 4)))
library(hopkins)
# Compute Hopkins Statistic
set.seed(123)
hopkins_stat <- hopkins(mds_coords)
# Print the Hopkins Statistic
print(paste("Hopkins Statistic:", round(hopkins_stat, 4)))
# Perform hierarchical clustering using Ward's method
hc_result <- hclust(distance_matrix, method = "ward.D2")
# Convert similarity matrix to a distance matrix
distance_matrix <- as.dist(1 - similarity_matrix) %>%
as.matrix()
# Perform hierarchical clustering using Ward's method
hc_result <- hclust(distance_matrix, method = "ward.D2")
# Convert similarity matrix to a distance matrix
distance_matrix <- as.dist(1 - similarity_matrix)
distance_matrix_full <- as.matrix(distance_matrix)
distance_matrix_full <- as.matrix(distance_matrix)
# Convert similarity matrix to a distance matrix
distance_matrix <- as.dist(1 - similarity_matrix)
distance_matrix_full <- as.matrix(distance_matrix)
# Multidimensional Scaling (MDS) for Visualization
mds_coords <- cmdscale(distance_matrix, k = 2)
# Compute Hopkins Statistic
set.seed(123)
hopkins_stat <- hopkins(mds_coords)
# Print the Hopkins Statistic
print(paste("Hopkins Statistic:", round(hopkins_stat, 4)))
# Reduction to 2D
kmeans_data <- data.frame(X1 = mds_coords[, 1], X2 = mds_coords[, 2])
# Elbow Method for K-Means
set.seed(123)
k_values <- 2:15
wss_means <- sapply(k_values, function(k) {
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
kmeans_result$tot.withinss
})
# Plot the Elbow Curve
plot(k_values, wss_means, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Total Within-Cluster Sum of Squares (WSS)",
main = "Elbow Method for K-Means")
# Silhouette Analysis for K-Means
silhouette_scores_means <- sapply(k_values, function(k) {
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
silhouette_result <- silhouette(kmeans_result$cluster, dist(kmeans_data[, 1:2]))
mean(silhouette_result[, 3])  # Average silhouette width
})
# Plot Silhouette Scores
plot(k_values, silhouette_scores_means, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for K-Means")
# Gap Statistic for K-Means
gap_stat_means <- clusGap(kmeans_data[, 1:2], FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat_means)
# Run K-means
k <- 6
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
# Add cluster labels to the dataset
kmeans_data$cluster <- as.factor(kmeans_result$cluster)
# Visualize the clusters
ggplot(kmeans_data, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "K-Means Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Multidimensional Scaling (MDS) for Visualization
mds_coords <- cmdscale(distance_matrix_full, k = 2)
# Reduction to 2D
kmeans_data <- data.frame(X1 = mds_coords[, 1], X2 = mds_coords[, 2])
# Run K-means
k <- 6
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
# Add cluster labels to the dataset
kmeans_data$cluster <- as.factor(kmeans_result$cluster)
# Visualize the clusters
ggplot(kmeans_data, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "K-Means Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Multidimensional Scaling (MDS) for Visualization
mds_coords <- cmdscale(distance_matrix, k = 2)
# Perform hierarchical clustering using Ward's method
hc_result <- hclust(distance_matrix, method = "ward.D2")
# Plot the dendrogram
plot(hc_result, main = "Agglomerative Hierarchical Clustering", sub = "", xlab = "", cex = 0.8)
# Cut the dendrogram into clusters
k <- 6
hc_clusters <- cutree(hc_result, k = num_clusters)
# Cut the dendrogram into clusters
k <- 6
hc_clusters <- cutree(hc_result, k = k)
# Add cluster labels to the dataset
commenter_jaccard$hc_cluster <- hc_clusters
# Visualize clusters using MDS
visualization_data_aggl <- data.frame(
X1 = mds_coords[, 1],
X2 = mds_coords[, 2],
cluster = as.factor(hc_clusters)
)
# Plot with channel names
ggplot(visualization_data_aggl, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),  # Add channel names
size = 3,  # Adjust text size
hjust = 0.5, vjust = -0.5,  # Position the labels slightly above the points
check_overlap = TRUE) +  # Prevent overlapping labels where possible
labs(
title = "Agglomerative Hierarchical Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal() +
theme(
legend.position = "bottom",
plot.title = element_text(hjust = 0.5)
)
# Store average silhouette width for each k
silhouette_scores <- sapply(k_values, function(k) {
kmedoids_result <- pam(distance_matrix, k)
silhouette_result <- silhouette(kmedoids_result$clustering, dist(distance_matrix))
mean(silhouette_result[, 3])  # Average silhouette width
})
# Plot Silhouette Scores
plot(k_values, silhouette_scores, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for K-Medoids")
# Gap statistic
gap_stat <- clusGap(distance_matrix_full, FUN = pam, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat)
# Inspect Gap Statistic
print(gap_stat)
# Choose the optimal k (based on the Elbow Method, or use silhouette width)
k <- 7  # probably 6 or 8 according to Elbow
# Perform K-Medoids clustering with the chosen number of clusters
kmedoids_result <- pam(distance_matrix, k = k)
# Extract cluster labels
kmedoids_clusters <- kmedoids_result$clustering
# Add cluster labels to the original data
commenter_jaccard$kmedoids_cluster <- kmedoids_clusters
# Prepare data for visualization
kmedoids_data <- data.frame(
X1 = mds_coords[, 1],
X2 = mds_coords[, 2],
cluster = as.factor(kmedoids_clusters),
title = commenter_jaccard$title
)
# Plot the K-Medoids clustering result
ggplot(kmedoids_data, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3, alpha = 0.8) +  # Points with transparency
geom_text(aes(label = title),        # Add channel names
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "K-Medoids Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal() +
theme(
legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
)
# Reduction to 2D
kmeans_data <- data.frame(X1 = mds_coords[, 1], X2 = mds_coords[, 2])
# Elbow Method for K-Means
set.seed(123)
k_values <- 2:15
wss_means <- sapply(k_values, function(k) {
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
kmeans_result$tot.withinss
})
# Plot the Elbow Curve
plot(k_values, wss_means, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Total Within-Cluster Sum of Squares (WSS)",
main = "Elbow Method for K-Means")
# Silhouette Analysis for K-Means
silhouette_scores_means <- sapply(k_values, function(k) {
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
silhouette_result <- silhouette(kmeans_result$cluster, dist(kmeans_data[, 1:2]))
mean(silhouette_result[, 3])  # Average silhouette width
})
# Plot Silhouette Scores
plot(k_values, silhouette_scores_means, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for K-Means")
# Gap Statistic for K-Means
gap_stat_means <- clusGap(kmeans_data[, 1:2], FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat_means)
# Run K-means
k <- 6
kmeans_result <- kmeans(kmeans_data[, 1:2], centers = k, nstart = 25)
# Add cluster labels to the dataset
kmeans_data$cluster <- as.factor(kmeans_result$cluster)
# Visualize the clusters
ggplot(kmeans_data, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "K-Means Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Gap statistic
gap_stat <- clusGap(distance_matrix, FUN = pam, K.max = 15, B = 50)
# Gap Statistic for K-Means
gap_stat_means <- clusGap(kmeans_data[, 1:2], FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat_means)
# Gap Statistic for K-Means
gap_stat_means <- clusGap(mds_coords, FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat_means)
# Gap Statistic for K-Means
gap_stat_means <- clusGap(kmeans_data[, 1:2], FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat_means)
kmeans_result <- kmeans(distance_matrix, centers = k, nstart = 25)
# Gap Statistic for K-Means
gap_stat_means <- clusGap(distance_matrix, FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat_means)
# Gap Statistic for K-Means
gap_stat_means <- clusGap(distance_matrix_full, FUN = kmeans, nstart = 25, K.max = 15, B = 50)
# Plot the Gap Statistic
fviz_gap_stat(gap_stat_means)
