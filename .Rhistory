random_effects <- VarCorr(final_model)
# PART 2d: Extract estimates for fixed parameters
fixed_effects
# PART 2f: Change in chosen information index (AIC comparison)
unconditional_aic <- AIC(unconditional_model)
final_model_aic <- AIC(final_model)
aic_change <- unconditional_aic - final_model_aic
# PART 2g: Relative change in first-level residual variance
residual_variance_change <- attr(VarCorr(unconditional_model), "sc")^2 - attr(VarCorr(final_model), "sc")^2
residual_variance_change_percent <- (residual_variance_change / attr(VarCorr(unconditional_model), "sc")^2) * 100
# Print results
list(
Fixed_Effects = fixed_effects,
Random_Effects = random_effects,
AIC_Change = aic_change,
Residual_Variance_Change_Percent = residual_variance_change_percent
)
# Fit the final HLM model
final_model <- lmer(mathach ~ ses + minority + meanses + (1 | school), data = school_data)
# Summary of the final model
summary(final_model)
# PART 2a and 2b: Written equations for both levels and combined model
# These need to be manually written based on the fixed and random effects.
# PART 2c: List fixed and random effect variables
fixed_effects <- fixef(final_model)
random_effects <- VarCorr(final_model)
# PART 2d: Extract estimates for fixed parameters
fixed_effects
# PART 2e: Write the combined model with parameter estimates (manually from fixed_effects output)
# PART 2f: Change in chosen information index (AIC comparison)
unconditional_aic <- AIC(unconditional_model)
final_model_aic <- AIC(final_model)
aic_change <- unconditional_aic - final_model_aic
# PART 2g: Relative change in first-level residual variance
residual_variance_change <- attr(VarCorr(unconditional_model), "sc")^2 - attr(VarCorr(final_model), "sc")^2
residual_variance_change_percent <- (residual_variance_change / attr(VarCorr(unconditional_model), "sc")^2) * 100
# Print results
list(
Fixed_Effects = fixed_effects,
Random_Effects = random_effects,
AIC_Change = aic_change,
Residual_Variance_Change_Percent = residual_variance_change_percent
)
# Fit the final HLM model
final_model <- lmer(mathach ~ ses + minority + size + (1 | school), data = school_data)
# Summary of the final model
summary(final_model)
# PART 2a and 2b: Written equations for both levels and combined model
# These need to be manually written based on the fixed and random effects.
# PART 2c: List fixed and random effect variables
fixed_effects <- fixef(final_model)
random_effects <- VarCorr(final_model)
# PART 2d: Extract estimates for fixed parameters
fixed_effects
# PART 2e: Write the combined model with parameter estimates (manually from fixed_effects output)
# PART 2f: Change in chosen information index (AIC comparison)
unconditional_aic <- AIC(unconditional_model)
final_model_aic <- AIC(final_model)
aic_change <- unconditional_aic - final_model_aic
# PART 2g: Relative change in first-level residual variance
residual_variance_change <- attr(VarCorr(unconditional_model), "sc")^2 - attr(VarCorr(final_model), "sc")^2
residual_variance_change_percent <- (residual_variance_change / attr(VarCorr(unconditional_model), "sc")^2) * 100
# Print results
list(
Fixed_Effects = fixed_effects,
Random_Effects = random_effects,
AIC_Change = aic_change,
Residual_Variance_Change_Percent = residual_variance_change_percent
)
# Fit the final HLM model
final_model <- lmer(mathach ~ ses + minority + pracad + (1 | school), data = school_data)
# Summary of the final model
summary(final_model)
# PART 2a and 2b: Written equations for both levels and combined model
# These need to be manually written based on the fixed and random effects.
# PART 2c: List fixed and random effect variables
fixed_effects <- fixef(final_model)
random_effects <- VarCorr(final_model)
# PART 2d: Extract estimates for fixed parameters
fixed_effects
# PART 2e: Write the combined model with parameter estimates (manually from fixed_effects output)
# PART 2f: Change in chosen information index (AIC comparison)
unconditional_aic <- AIC(unconditional_model)
final_model_aic <- AIC(final_model)
aic_change <- unconditional_aic - final_model_aic
# PART 2g: Relative change in first-level residual variance
residual_variance_change <- attr(VarCorr(unconditional_model), "sc")^2 - attr(VarCorr(final_model), "sc")^2
residual_variance_change_percent <- (residual_variance_change / attr(VarCorr(unconditional_model), "sc")^2) * 100
# Print results
list(
Fixed_Effects = fixed_effects,
Random_Effects = random_effects,
AIC_Change = aic_change,
Residual_Variance_Change_Percent = residual_variance_change_percent
)
# Correlation matrix for predictors in the proposed model
cor(school_data[c("ses", "minority", "pracad")])
# Check multicollinearity
library(car)
vif(final_model)
## Part 3
# Forecasting using the proposed model
forecast_data <- data.frame(
ses = 0,
minority = 1,
pracad = 0.25
)
# Predict math achievement
forecast <- predict(final_model, newdata = forecast_data, re.form = NA) # Fixed effects only
forecast
summary(school_data)
##############################
#
# Multivariate Statistics
# 2024-11-21
# Test 5
#
##############################
### Exercise 1 ----------------------------------
library(haven)
school_data <- read_sav("hsbdataset.sav")
head(school_data)
summary(school_data)
## Part 1
# Fit the unconditional model
library(Matrix)
library(lme4)
unconditional_model <- lmer(mathach ~ 1 + (1 | school), data = school_data)
# Summary of the model
summary(unconditional_model)
# Calculate ICC
icc <- as.numeric(VarCorr(unconditional_model)$school[1]) /
(as.numeric(VarCorr(unconditional_model)$school[1]) + attr(VarCorr(unconditional_model), "sc")^2)
icc
## Part 2
# Fit the final HLM model
final_model <- lmer(mathach ~ ses + minority + pracad + (1 | school), data = school_data)
# Summary of the final model
summary(final_model)
# Correlation matrix for predictors
cor(school_data[c("ses", "minority", "pracad")])
# PART 2a and 2b: Written equations for both levels and combined model
# These need to be manually written based on the fixed and random effects.
# PART 2c: List fixed and random effect variables
fixed_effects <- fixef(final_model)
random_effects <- VarCorr(final_model)
# PART 2d: Extract estimates for fixed parameters
fixed_effects
# PART 2e: Write the combined model with parameter estimates (manually from fixed_effects output)
# PART 2f: Change in chosen information index (AIC comparison)
unconditional_aic <- AIC(unconditional_model)
final_model_aic <- AIC(final_model)
aic_change <- unconditional_aic - final_model_aic
# PART 2g: Relative change in first-level residual variance
residual_variance_change <- attr(VarCorr(unconditional_model), "sc")^2 - attr(VarCorr(final_model), "sc")^2
residual_variance_change_percent <- (residual_variance_change / attr(VarCorr(unconditional_model), "sc")^2) * 100
# Print results
list(
Fixed_Effects = fixed_effects,
Random_Effects = random_effects,
AIC_Change = aic_change,
Residual_Variance_Change_Percent = residual_variance_change_percent
)
## Part 3
# Forecasting using the proposed model
forecast_data <- data.frame(
ses = 0,
minority = 1,
pracad = 0.25
)
# Predict math achievement
forecast <- predict(final_model, newdata = forecast_data, re.form = NA) # Fixed effects only
forecast
school_data <- read_sav("hsbdataset.sav")
# PART 2d: Extract estimates for fixed parameters
fixed_effects
# PART 2c: List fixed and random effect variables
fixed_effects <- fixef(final_model)
random_effects <- VarCorr(final_model)
# PART 2d: Extract estimates for fixed parameters
fixed_effects
random_effects
tinytex::install_tinytex()
tinytex::uninstall_tinytex()
tinytex::uninstall_tinytex()
tinytex::install_tinytex()
# Libraries
library(readr)
library(cluster)
library(ggplot2)
library(dbscan)
library(kernlab)
library(dplyr)
library(igraph)
library(mclust)
library(data.table)
library(factoextra)
library(plotly)
library(hopkins)
library(Rtsne)
library(uwot)
library(pheatmap)
library(dendextend)
# Load the data (Matas)
setwd("C:/Users/DELL i5/OneDrive/Desktop/Data Mining/data-mining")
channel_info <- read_csv("data/channel_info.csv")
commenter_jaccard <- read_csv("data/comment_jaccard_matrix.csv")
############## Similarity and Distance Matrix Preparation ##############
# Define similarity matrix
similarity_matrix <- as.matrix(commenter_jaccard[, -1])
# Ensure the similarity matrix is symmetric
similarity_matrix <- (similarity_matrix + t(similarity_matrix)) / 2
# Remove dimnames to avoid warnings
rownames(similarity_matrix) <- NULL
colnames(similarity_matrix) <- NULL
# Convert similarity matrix to a distance matrix
distance_matrix <- as.dist(1 - similarity_matrix)
distance_matrix_full <- as.matrix(distance_matrix)
# Multidimensional Scaling (MDS) for Visualization
mds_coords <- cmdscale(distance_matrix_full, k = 2)
# Compute Hopkins Statistic
set.seed(123)
hopkins_stat <- hopkins(mds_coords)
print(paste("Hopkins Statistic:", round(hopkins_stat, 4)))
# Perform divisive clustering
divisive_result <- diana(distance_matrix)
divisive_result
# Plot the dendrogram
plot(as.hclust(divisive_result),
main = "Divisive Hierarchical Clustering",
sub = "",
xlab = "",
cex = 0.8)
k_values <- 2:15
# Compute silhouette scores for divisive clustering
silhouette_scores_div <- sapply(k_values, function(k) {
clusters <- cutree(as.hclust(divisive_result), k)
silhouette_result <- silhouette(clusters, distance_matrix)
mean(silhouette_result[, 3], na.rm = TRUE)
})
# Plot silhouette scores
plot(k_values, silhouette_scores_div, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for Divisive Clustering")
# Optimal number of clusters based on silhouette method
optimal_k_silhouette <- k_values[which.max(silhouette_scores_div)]
print(paste("Optimal number of clusters (Silhouette):", optimal_k_silhouette))
k_values <- 2:15
# Compute silhouette scores for divisive clustering
silhouette_scores_div <- sapply(k_values, function(k) {
clusters <- cutree(as.hclust(divisive_result), k)
silhouette_result <- silhouette(clusters, distance_matrix)
mean(silhouette_result[, 3])
})
# Plot silhouette scores
plot(k_values, silhouette_scores_div, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for Divisive Clustering")
# Optimal number of clusters based on silhouette method
optimal_k_silhouette <- k_values[which.max(silhouette_scores_div)]
print(paste("Optimal number of clusters (Silhouette):", optimal_k_silhouette))
# Compute silhouette scores for divisive clustering
silhouette_scores_div <- sapply(k_values, function(k) {
clusters <- cutree(as.hclust(divisive_result), k)
silhouette_result <- silhouette(clusters, distance_matrix_full)
mean(silhouette_result[, 3], na.rm = TRUE)
})
# Plot silhouette scores
plot(k_values, silhouette_scores_div, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for Divisive Clustering")
# Optimal number of clusters based on silhouette method
optimal_k_silhouette <- k_values[which.max(silhouette_scores_div)]
print(paste("Optimal number of clusters (Silhouette):", optimal_k_silhouette))
# Compute WSS for divisive clustering
wss_div <- sapply(k_values, function(k) {
clusters <- cutree(as.hclust(divisive_result), k)
cluster_wss <- sapply(unique(clusters), function(cluster) {
cluster_points <- which(clusters == cluster)
cluster_distances <- distance_matrix_full[cluster_points, cluster_points]
mean(as.matrix(cluster_distances)) * length(cluster_points)
})
sum(cluster_wss)
})
# Plot the Elbow Curve
plot(k_values, wss_div, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Total Within-Cluster Sum of Squares (WSS)",
main = "Elbow Method for Divisive Clustering")
# Cut the dendrogram into 6 clusters
k <- 6
divisive_clusters <- cutree(as.hclust(divisive_result), k = k)
# Add cluster labels to the dataset
commenter_jaccard$divisive_cluster <- divisive_clusters
# Visualize the clusters using MDS
visualization_data_div <- data.frame(
X1 = mds_coords[, 1],
X2 = mds_coords[, 2],
cluster = as.factor(divisive_clusters)
)
# Plot with channel names
ggplot(visualization_data_div, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +  # Plot the points
geom_text(aes(label = commenter_jaccard$title),  # Add channel names
size = 3,  # Adjust text size
hjust = 0.5, vjust = -0.5,  # Position the labels slightly above the points
check_overlap = TRUE) +  # Avoid label overlap
labs(
title = "Divisive Hierarchical Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal() +
theme(
legend.position = "bottom",  # Move legend to bottom
plot.title = element_text(hjust = 0.5)  # Center-align title
)
# Plot BIC values for different numbers of clusters
plot(phc_result, what = "BIC", main = "BIC for Probabilistic Hierarchical Clustering")
# Set the number of clusters manually (otherwise it's equal to 1)
phc_result <- Mclust(distance_matrix_full, G = 6)
summary(phc_result)
# Plot BIC values for different numbers of clusters
plot(phc_result, what = "BIC", main = "BIC for Probabilistic Hierarchical Clustering")
k_values <- 2:15
# Compute silhouette scores for probabilistic clustering
silhouette_scores_phc <- sapply(k_values, function(k) {
phc_temp <- Mclust(data_matrix, G = k)
clusters <- phc_temp$classification
silhouette_result <- silhouette(clusters, distance_matrix)
mean(silhouette_result[, 3], na.rm = TRUE)
})
k_values <- 2:15
# Compute silhouette scores for probabilistic clustering
silhouette_scores_phc <- sapply(k_values, function(k) {
phc_temp <- Mclust(distance_matrix_full, G = k)
clusters <- phc_temp$classification
silhouette_result <- silhouette(clusters, distance_matrix)
mean(silhouette_result[, 3], na.rm = TRUE)
})
# Plot silhouette scores
plot(k_values, silhouette_scores_phc, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for Probabilistic Clustering")
# Compute silhouette scores for probabilistic clustering
silhouette_scores_phc <- sapply(k_values, function(k) {
phc_temp <- Mclust(distance_matrix_full, G = k)
clusters <- phc_temp$classification
silhouette_result <- silhouette(clusters, distance_matrix_full)
mean(silhouette_result[, 3], na.rm = TRUE)
})
# Plot silhouette scores
plot(k_values, silhouette_scores_phc, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)",
ylab = "Average Silhouette Score",
main = "Silhouette Method for Probabilistic Clustering")
# Add cluster labels to the dataset
commenter_jaccard$phc_cluster <- phc_result$classification
# Visualize the clustering results using MDS
visualization_data_prob <- data.frame(
X1 = mds_coords[, 1],
X2 = mds_coords[, 2],
cluster = as.factor(phc_result$classification)
)
ggplot(visualization_data_prob, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "Probabilistic Hierarchical Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
# Set the number of clusters manually (otherwise it's equal to 1)
phc_result <- Mclust(distance_matrix_full)
summary(phc_result)
# Plot BIC values for different numbers of clusters
plot(phc_result, what = "BIC", main = "BIC for Probabilistic Hierarchical Clustering")
k_values <- 2:15
# Symmetrize and normalize the similarity matrix
symmetric_similarity_matrix <- (similarity_matrix + t(similarity_matrix)) / 2
symmetric_similarity_matrix <- symmetric_similarity_matrix / max(symmetric_similarity_matrix)
dimnames(symmetric_similarity_matrix) <- NULL
k_values <- 2:15
# Symmetrize and normalize the similarity matrix
symmetric_similarity_matrix <- (similarity_matrix + t(similarity_matrix)) / 2
symmetric_similarity_matrix <- symmetric_similarity_matrix / max(symmetric_similarity_matrix)
dimnames(symmetric_similarity_matrix) <- NULL
# Eigen Gap Method
laplacian <- diag(rowSums(symmetric_similarity_matrix)) - symmetric_similarity_matrix  # Unnormalized Laplacian
eigen_result <- eigen(laplacian, symmetric = TRUE)
eigenvalues <- sort(eigen_result$values, decreasing = FALSE)
# Plot the eigenvalues to visualize the eigen gap
plot(eigenvalues[1:15], type = "b", pch = 19, xlab = "Index", ylab = "Eigenvalue",
main = "Eigenvalues of Laplacian (Eigen Gap Method)")
eigen_gap <- diff(eigenvalues[1:15])
optimal_k_eigen <- which.max(eigen_gap) + 1
cat("Optimal number of clusters (Eigen Gap):", optimal_k_eigen, "\n")
# Silhouette Method
k_values <- 2:15
silhouette_scores_spec <- sapply(k_values, function(k) {
spectral_result <- specc(symmetric_similarity_matrix, centers = k)
silhouette_result <- silhouette(as.integer(spectral_result@.Data), dist(symmetric_similarity_matrix))
mean(silhouette_result[, 3], na.rm = TRUE)
})
# Plot Silhouette Scores
plot(k_values, silhouette_scores_spec, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)", ylab = "Average Silhouette Score",
main = "Silhouette Method for Spectral Clustering")
optimal_k_silhouette <- k_values[which.max(silhouette_scores_spec)]
cat("Optimal number of clusters (Silhouette):", optimal_k_silhouette, "\n")
optimal_k_silhouette
k <- optimal_k_silhouette
# Perform spectral clustering (first time - error, second time it works) as nezinau, kas cia per xujne naxui
spectral_result <- specc(symmetric_similarity_matrix, centers = k)
spectral_result <- specc(symmetric_similarity_matrix, centers = k)
# Add cluster labels to the dataset
commenter_jaccard$spectral_cluster <- as.factor(spectral_result@.Data)
# Visualize the clusters using MDS
visualization_data_spec <- data.frame(
X1 = mds_coords[, 1],
X2 = mds_coords[, 2],
cluster = commenter_jaccard$spectral_cluster
)
# Plot the clustering result with channel names
ggplot(visualization_data_spec, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +  # Plot the points
geom_text(aes(label = commenter_jaccard$title),  # Add channel names
size = 3,  # Adjust text size
hjust = 0.5, vjust = -0.5,  # Position labels slightly above the points
check_overlap = TRUE) +  # Avoid label overlap
labs(
title = "Spectral Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal() +
theme(
legend.position = "bottom",  # Move legend to the bottom
plot.title = element_text(hjust = 0.5)  # Center-align title
)
# Create a graph from the Jaccard similarity matrix
graph <- similarity_matrix %>%
graph_from_adjacency_matrix(mode = "undirected", weighted = TRUE, diag = FALSE)
# Perform community detection using a local clustering algorithm
# Here, we use the Louvain method (fast and widely used)
community_result <- cluster_louvain(graph)
# Extract cluster memberships
cluster_memberships <- membership(community_result)
# Add cluster labels to the dataset
commenter_jaccard$graph_cluster <- cluster_memberships
# Visualize the graph with clusters
layout <- layout_with_fr(graph)  # Fruchterman-Reingold layout for visualization
plot(graph, layout = layout,
vertex.color = cluster_memberships,
vertex.label = commenter_jaccard$title,  # Add channel names as labels
vertex.label.cex = 0.7,  # Adjust label size
vertex.label.color = "black",  # Set label color
vertex.size = 20,  # Increase node size for better visibility
edge.color = "gray",  # Make edges less visually dominant
edge.width = 0.5,  # Thin out edges for less clutter
main = "Graph-Based Clustering")
# MDS-based visualization
visualization_data_grph <- data.frame(
X1 = mds_coords[, 1],
X2 = mds_coords[, 2],
cluster = as.factor(cluster_memberships)
)
ggplot(visualization_data_grph, aes(x = X1, y = X2, color = cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3,
hjust = 0.5, vjust = -0.5,
check_overlap = TRUE) +
labs(
title = "Graph-Based Clustering with Channel Names",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal() +
theme(
legend.position = "bottom",
plot.title = element_text(hjust = 0.5)
)
cluster_memberships
community_result
graph
layout
# Use MDS for reducing dimensionality to 2D (if needed for visualization)
grid_data <- data.frame(X1 = mds_coords[, 1], X2 = mds_coords[, 2])
# Define the grid
grid_size <- 6  # Adjust the grid resolution (higher = finer grid)
grid_data$grid_x <- cut(grid_data$X1, breaks = grid_size, labels = FALSE)
grid_data$grid_y <- cut(grid_data$X2, breaks = grid_size, labels = FALSE)
# Assign cluster IDs based on grid cells
grid_data$grid_cluster <- as.factor(paste(grid_data$grid_x, grid_data$grid_y, sep = "-"))
# Visualize the grid-based clustering
ggplot(grid_data, aes(x = X1, y = X2, color = grid_cluster)) +
geom_point(size = 3) +
geom_text(aes(label = commenter_jaccard$title),
size = 3, hjust = 0.5, vjust = -0.5, check_overlap = TRUE) +
labs(
title = "Grid-Based Clustering",
x = "MDS Dimension 1",
y = "MDS Dimension 2"
) +
theme_minimal()
